import asyncio
import logging
import os
from typing import Dict, Optional, Any, Union, List, Tuple
import numpy as np
import faiss
from pathlib import Path
from openai import OpenAI
from flask import current_app
from models import BibleVerse
from sqlalchemy import text
from datetime import datetime

# Configuraci칩n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NevinUtils:
    """Clase de utilidades para el asistente Nevin con b칰squeda local y an치lisis teol칩gico."""

    def __init__(self, app=None):
        """Inicializa OpenAI y carga los 칤ndices FAISS locales."""
        self.app = app
        self.client = None
        self.faiss_indexes = {}  # Almacena los 칤ndices FAISS por categor칤a
        self.faiss_data = {}     # Almacena los datos asociados a cada 칤ndice
        self.embeddings_cache = {}  # Cache para embeddings frecuentes

        # Inicializar componentes
        self._init_openai()
        self._load_faiss_indexes()
        logger.info("NevinUtils inicializado con 칠xito")

    def _init_openai(self):
        """Inicializa el cliente OpenAI con manejo de errores."""
        try:
            openai_key = os.getenv('OPENAI_API_KEY')
            if not openai_key:
                logger.error("OPENAI_API_KEY no est치 configurada")
                return

            self.client = OpenAI(api_key=openai_key)
            logger.info("Cliente OpenAI inicializado correctamente")

        except Exception as e:
            logger.error(f"Error inicializando OpenAI client: {str(e)}")

    def _load_faiss_indexes(self):
        """Carga los 칤ndices FAISS y sus datos asociados desde la carpeta nevin_knowledge."""
        try:
            knowledge_paths = ['nevin_knowledge', 'Nevin_AI/nevin_knowledge']
            faiss_files = []
            for path in knowledge_paths:
                knowledge_dir = Path(path)
                if knowledge_dir.exists():
                    faiss_files.extend(knowledge_dir.glob("*.faiss"))

            logger.info(f"Encontrados {len(faiss_files)} archivos FAISS en {knowledge_paths}")

            for index_file in faiss_files:
                try:
                    # Extraer nombre base del archivo
                    base_name = index_file.stem

                    # Cargar 칤ndice FAISS
                    logger.info(f"Cargando 칤ndice: {index_file}")
                    index = faiss.read_index(str(index_file))

                    # Verificar que el 칤ndice est칠 entrenado
                    if not index.is_trained:
                        logger.warning(f"칈ndice {base_name} no est치 entrenado")
                        continue

                    self.faiss_indexes[base_name] = index

                    # Cargar datos asociados
                    data_file = index_file.with_suffix('.npy')
                    if data_file.exists():
                        self.faiss_data[base_name] = np.load(str(data_file), allow_pickle=True)
                        logger.info(f"Datos cargados para {base_name}: {len(self.faiss_data[base_name])} registros")
                    else:
                        logger.warning(f"No se encontr칩 archivo de datos para {base_name}")

                except Exception as e:
                    logger.error(f"Error cargando 칤ndice {index_file}: {str(e)}")
                    continue

            if not self.faiss_indexes:
                logger.warning("No se cargaron 칤ndices FAISS. El sistema funcionar치 con capacidades reducidas.")
                return

            logger.info(f"Carga exitosa: {len(self.faiss_indexes)} 칤ndices FAISS")

        except Exception as e:
            logger.warning(f"Error cargando 칤ndices FAISS: {str(e)}")
            logger.info("El sistema continuar치 funcionando con capacidades reducidas")

    async def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """Genera embedding usando OpenAI de forma as칤ncrona."""
        try:
            if not self.client:
                raise ValueError("Cliente OpenAI no inicializado")

            # Verificar cache
            if text in self.embeddings_cache:
                return self.embeddings_cache[text]

            response = await asyncio.to_thread(
                lambda: self.client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=text
                )
            )

            if not response.data or not response.data[0].embedding:
                raise ValueError("No se recibi칩 embedding en la respuesta")

            embedding = response.data[0].embedding

            # Guardar en cache
            self.embeddings_cache[text] = embedding

            return embedding

        except Exception as e:
            logger.error(f"Error generando embedding: {str(e)}")
            return None

    async def format_combined_results(self, combined_results: Dict[str, Any]) -> str:
        """Formatea los resultados combinados de b칰squeda."""
        try:
            response = []

            # Formatear vers칤culos b칤blicos
            if combined_results.get('bible_verses'):
                response.append("游닀 Vers칤culos B칤blicos:")
                for verse in combined_results['bible_verses']:
                    response.append(f"__{verse['reference']}__ - {verse['content']}")
                    if verse.get('content_tzotzil'):
                        response.append(f"(Tzotzil: {verse['content_tzotzil']})")

            # Formatear contenido teol칩gico
            if combined_results.get('theological_content'):
                response.append("\n游닄 Comentarios Teol칩gicos:")
                for comment in combined_results['theological_content']:
                    if comment.get('source'):
                        response.append(f"({comment['source']}) - {comment['content']}")
                    else:
                        response.append(f"{comment['content']}")

            return "\n".join(response)

        except Exception as e:
            logger.error(f"Error formateando resultados: {str(e)}")
            return "Lo siento, hubo un error al formatear los resultados."

    def search_bible(self, query: str, limit: int = 5) -> List[Dict]:
        """Busca vers칤culos relevantes en la base de datos SQLite con logging detallado."""
        try:
            logger.info(f"Iniciando b칰squeda b칤blica para query: '{query}'")
            
            if not self.app:
                raise ValueError("App context no inicializado")
                
            with self.app.app_context():
                logger.info("Ejecutando consulta en base de datos...")
                # Realizar b칰squeda por similitud sem치ntica
                verses = BibleVerse.query.filter(
                    BibleVerse.spanish_text.ilike(f"%{query}%") |
                    BibleVerse.tzotzil_text.ilike(f"%{query}%")
                ).limit(limit).all()
                
                logger.info(f"Encontrados {len(verses)} vers칤culos en la base de datos")
            
                results = []
                for verse in verses:
                    result = {
                        'content': verse.spanish_text,
                        'content_tzotzil': verse.tzotzil_text,
                        'reference': f"{verse.book} {verse.chapter}:{verse.verse}",
                        'score': 1.0,  # Score base para resultados directos
                        'type': 'bible'
                    }
                    results.append(result)
                    logger.debug(f"Agregado vers칤culo: {result['reference']}")
            
                logger.info(f"B칰squeda b칤blica completada exitosamente. Retornando {len(results)} resultados")
                return results
        
        except Exception as e:
            error_msg = f"Error en b칰squeda b칤blica: {str(e)}"
            logger.error(error_msg, exc_info=True)
            logger.error("Detalles del error:", {
                'query': query,
                'error_type': type(e).__name__,
                'app_context': bool(self.app)
            })
            return []

    def search_theological(self, query: str, threshold: float = 0.7) -> List[Dict]:
        """Realiza b칰squeda en 칤ndices FAISS locales."""
        try:
            query_embedding = self._generate_embedding(query)
            if not query_embedding:
                return []

            results = []
            query_vector = np.array(query_embedding).reshape(1, -1).astype('float32')

            for index_name, index in self.faiss_indexes.items():
                try:
                    D, I = index.search(query_vector, k=5)
                    
                    if index_name in self.faiss_data:
                        data = self.faiss_data[index_name]
                        for i, (distance, idx) in enumerate(zip(D[0], I[0])):
                            if idx < len(data) and distance < threshold:
                                score = 1.0 - (distance / 2.0)
                                results.append({
                                    'content': str(data[idx]),
                                    'source': index_name,
                                    'score': float(score),
                                    'type': 'theological'
                                })

                except Exception as e:
                    logger.error(f"Error buscando en 칤ndice {index_name}: {str(e)}")
                    continue

            return sorted(results, key=lambda x: x['score'], reverse=True)

        except Exception as e:
            logger.error(f"Error en b칰squeda teol칩gica: {str(e)}")
            return []

    def generate_response(self, query: str, context: Dict[str, Any]) -> str:
        """Genera una respuesta estructurada usando GPT-4."""
        try:
            # Construir el prompt con el contexto
            prompt = self._build_prompt(query, context)

            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system", 
                        "content": """Eres Nevin, un asistente teol칩gico experto que proporciona respuestas 
                        profundas y elegantes, equilibrando claridad para principiantes con profundidad para expertos.
                        Usa las referencias proporcionadas para dar respuestas precisas y bien fundamentadas."""
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"Error generando respuesta: {str(e)}")
            return "Lo siento, hubo un error al generar la respuesta. Por favor, intenta de nuevo."

    def _build_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Construye el prompt para GPT-4 con el contexto proporcionado."""
        prompt = f"""Como Nevin, responde a la siguiente pregunta teol칩gica:
        
        Pregunta: {query}
        
        Contexto B칤blico:
        """
        # A침adir vers칤culos b칤blicos
        for verse in context.get('bible_verses', []):
            prompt += f"- {verse['reference']}: {verse['content']}\n"

        prompt += "\nContexto Teol칩gico:\n"
        # A침adir comentarios teol칩gicos
        for comment in context.get('theological_content', []):
            prompt += f"- Fuente ({comment['source']}): {comment['content']}\n"

        prompt += """
        Por favor, estructura tu respuesta as칤:
        1. Introducci칩n Teol칩gica (2-3 oraciones)
        2. Fundamento B칤blico (citando vers칤culos relevantes)
        3. Explicaci칩n Teol칩gica (para principiantes y expertos)
        4. Aplicaci칩n Pr치ctica
        
        Formato para referencias:
        - Vers칤culos: __Referencia__ - Texto
        - Comentarios: (Fuente) - Contenido
        """

        return prompt

    def process_query(self, query: str) -> Dict[str, Any]:
        """Procesa una consulta teol칩gica completa con logging detallado."""
        try:
            logger.info(f"Iniciando procesamiento de consulta: '{query}'")
            
            # 1. Buscar vers칤culos relevantes
            logger.info("Buscando vers칤culos b칤blicos relevantes...")
            bible_results = self.search_bible(query)
            logger.info(f"Encontrados {len(bible_results)} vers칤culos relevantes")
            
            # 2. Buscar contenido teol칩gico
            logger.info("Consultando base de conocimiento teol칩gico (FAISS)...")
            theological_results = self.search_theological(query)
            logger.info(f"Encontrados {len(theological_results)} resultados teol칩gicos")
            
            # 3. Preparar contexto para la respuesta
            logger.info("Preparando contexto para generaci칩n de respuesta...")
            context = {
                'bible_verses': bible_results,
                'theological_content': theological_results
            }
            
            # 4. Generar respuesta
            logger.info("Generando respuesta con GPT-4...")
            response = self.generate_response(query, context)
            logger.info("Respuesta generada exitosamente")
            
            result = {
                'success': True,
                'response': response,
                'sources': {
                    'bible': bible_results,
                    'theological': theological_results
                }
            }
            logger.info("Procesamiento completado exitosamente")
            return result
            
        except Exception as e:
            error_msg = f"Error procesando consulta: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'error': error_msg,
                'details': {
                    'error_type': type(e).__name__,
                    'error_location': 'process_query'
                }
            }
    async def format_combined_results(self, combined_results: Dict[str, Any]) -> str:
        """
        Formatea los resultados combinados de b칰squeda de forma as칤ncrona.

        Args:
            combined_results: Diccionario con resultados de b칰squeda

        Returns:
            Respuesta formateada
        """
        try:
            response = []

            # Formatear vers칤culos b칤blicos
            if combined_results.get('bible_verses'):
                response.append("游닀 Vers칤culos B칤blicos:")
                for verse in combined_results['bible_verses']:
                    response.append(f"__{verse['reference']}__ - {verse['content']}")
                    if verse.get('content_tzotzil'):
                        response.append(f"(Tzotzil: {verse['content_tzotzil']})")

            # Formatear contenido teol칩gico
            if combined_results.get('theological_content'):
                response.append("\n游닄 Comentarios Teol칩gicos:")
                for comment in combined_results['theological_content']:
                    if comment.get('source'):
                        response.append(f"({comment['source']}) - {comment['content']}")
                    else:
                        response.append(f"{comment['content']}")

            return "\n".join(response)

        except Exception as e:
            logger.error(f"Error formateando resultados: {str(e)}")
            return "Lo siento, hubo un error al formatear los resultados."